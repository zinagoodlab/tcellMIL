{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f48af6-62aa-406b-972a-a8d64792f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                               f1_score, roc_auc_score, confusion_matrix)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import anndata\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "\n",
    "\n",
    "# Training function for the autoencoder\n",
    "def train_autoencoder(train_loader, val_loader, input_dim, latent_dim=32, num_epochs=100, \n",
    "                      learning_rate=5e-4, weight_decay=1e-4, patience=10, save_path='models'):\n",
    "    \"\"\"\n",
    "    Train the autoencoder model\n",
    "    \n",
    "    Parameters:\n",
    "    - train_loader: DataLoader for training data\n",
    "    - val_loader: DataLoader for validation data\n",
    "    - input_dim: Input dimension (number of TFs)\n",
    "    - latent_dim: Dimension of latent space\n",
    "    - num_epochs: Maximum number of training epochs\n",
    "    - learning_rate: Learning rate for optimizer\n",
    "    - weight_decay: L2 regularization strength\n",
    "    - patience: Early stopping patience\n",
    "    - save_path: Directory to save model and plots\n",
    "    \n",
    "    Returns:\n",
    "    - model: Trained autoencoder model\n",
    "    - train_losses: List of training losses per epoch\n",
    "    - val_losses: List of validation losses per epoch\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = Autoencoder(input_dim, latent_dim)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on: {device}\")\n",
    "    model.to(device)\n",
    "    \n",
    "\n",
    "    # Initialize WandB\n",
    "    # wandb.init(project=\"car-t-IP-MIL\", name=\"autoencoder-training\")\n",
    "    \n",
    "    # Training setup\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, _ in train_loader:\n",
    "            data = data.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # wandb.log({\"train_loss\": train_loss})\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data, _ in val_loader:\n",
    "                data = data.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, data)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        # wandb.log({\"val_loss\": val_loss})\n",
    "        \n",
    "        # Learning rate adjustment\n",
    "        scheduler.step() #val_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), f'{save_path}/best_autoencoder_simplified.pth')\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(f'{save_path}/best_autoencoder_simplified.pth'))\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(train_losses, label='Training Loss')\n",
    "    # plt.plot(val_losses, label='Validation Loss')\n",
    "    # plt.xlabel('Epochs')\n",
    "    # plt.ylabel('Loss')\n",
    "    # plt.title('Autoencoder Training and Validation Loss')\n",
    "    # plt.legend()\n",
    "    # plt.savefig(f'{save_path}/autoencoder_training_loss.png')\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# 5. Evaluation function\n",
    "def evaluate_autoencoder(model, test_loader, adata, tf_names, save_path='results'):\n",
    "    \"\"\"\n",
    "    Evaluate the trained autoencoder\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained autoencoder model\n",
    "    - test_loader: DataLoader for test data\n",
    "    - adata: Original AnnData object\n",
    "    - tf_names: Names of transcription factors\n",
    "    - save_path: Directory to save evaluation results\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Test set evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    all_inputs = []\n",
    "    all_reconstructions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Collect inputs and reconstructions for later analysis\n",
    "            all_inputs.append(data.cpu().numpy())\n",
    "            all_reconstructions.append(outputs.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.6f}')\n",
    "    # wandb.log({\"test_loss\": test_loss})\n",
    "    \n",
    "    # Combine batches\n",
    "    all_inputs = np.vstack(all_inputs)\n",
    "    all_reconstructions = np.vstack(all_reconstructions)\n",
    "    \n",
    "    # Calculate reconstruction error for each TF\n",
    "    mse_per_tf = np.mean((all_inputs - all_reconstructions)**2, axis=0)\n",
    "    \n",
    "    # Plot reconstruction error per TF\n",
    "    # plt.figure(figsize=(14, 6))\n",
    "    # plt.bar(range(len(mse_per_tf)), mse_per_tf)\n",
    "    # plt.xticks(range(len(mse_per_tf)), tf_names, rotation=90)\n",
    "    # plt.xlabel('Transcription Factors')\n",
    "    # plt.ylabel('MSE')\n",
    "    # plt.title('Reconstruction Error per Transcription Factor')\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f'{save_path}/tf_reconstruction_error.png')\n",
    "    \n",
    "    # Generate latent space representation for all cells\n",
    "    all_cells = adata.X\n",
    "    if isinstance(all_cells, np.ndarray) == False:\n",
    "        all_cells = all_cells.toarray()\n",
    "    \n",
    "    all_cells_tensor = torch.FloatTensor(all_cells).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        latent_vectors = model.encode(all_cells_tensor).cpu().numpy()\n",
    "    \n",
    "    # Create a new AnnData object with latent representations\n",
    "    adata_latent = sc.AnnData(latent_vectors)\n",
    "    adata_latent.obs = adata.obs.copy()  # Copy cell annotations\n",
    "    \n",
    "    # UMAP visualization of latent space\n",
    "    sc.pp.neighbors(adata_latent, use_rep='X')\n",
    "    sc.tl.umap(adata_latent)\n",
    "    \n",
    "    # os.makedirs('figures/umapresults', exist_ok=True)\n",
    "    # if 'patient_id' in adata_latent.obs.columns:\n",
    "    #     sc.pl.umap(adata_latent, color=['patient_id'], save=f'{save_path}/latent_umap_by_patient.png')\n",
    "    \n",
    "    # if 'Response_3m' in adata_latent.obs.columns:\n",
    "    #     sc.pl.umap(adata_latent, color=['Response_3m'], save=f'{save_path}/latent_umap_by_response.png')\n",
    "\n",
    "    \n",
    "    if 'patient_id' in adata_latent.obs.columns:\n",
    "        fig = sc.pl.umap(adata_latent, color=['patient_id'], show=False, return_fig=True)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        fig.savefig(f'{save_path}/latent_umap_by_patient.png', bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "    \n",
    "    if 'Response_3m' in adata_latent.obs.columns:\n",
    "        fig = sc.pl.umap(adata_latent, color=['Response_3m'], show=False, return_fig=True)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        fig.savefig(f'{save_path}/latent_umap_by_response.png', bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "    \n",
    "    \n",
    "    # Save latent representation for MIL\n",
    "    adata_latent.write(f'{save_path}/latent_representation.h5ad')\n",
    "    \n",
    "    return adata_latent, test_loss\n",
    "\n",
    "# 6. Main execution function\n",
    "def main(file_path, latent_dim=48, num_epochs=100):\n",
    "    \"\"\"\n",
    "    Main function to run the entire pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to h5ad file with SCENIC results\n",
    "    - latent_dim: Dimension of latent space\n",
    "    - num_epochs: Number of training epochs\n",
    "    \"\"\"\n",
    "    # Load and explore data\n",
    "    adata = load_and_explore_data(file_path)\n",
    "    \n",
    "    # Extract TF names\n",
    "    tf_names = adata.var_names.tolist()\n",
    "    \n",
    "    # Preprocess data\n",
    "    train_loader, val_loader, test_loader, input_dim = preprocess_data(adata)\n",
    "    \n",
    "    # Train autoencoder\n",
    "    model, train_losses, val_losses = train_autoencoder(\n",
    "        train_loader, val_loader, input_dim, latent_dim, num_epochs\n",
    "    )\n",
    "    \n",
    "    # Evaluate autoencoder\n",
    "    adata_latent, test_loss = evaluate_autoencoder(model, test_loader, adata, tf_names)\n",
    "    \n",
    "    print(\"Autoencoder training and evaluation complete!\")\n",
    "    print(f\"Final test loss: {test_loss:.6f}\")\n",
    "    print(f\"Latent representation shape: {adata_latent.shape}\")\n",
    "    print(\"Latent representation saved for MIL implementation.\")\n",
    "    \n",
    "    return model, adata_latent\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6439e299-1447-435f-b134-72cb3acb9482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# 1. MIL Dataset class for patient bags\n",
    "class PatientBagDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Multiple Instance Learning where each bag contains cells from one patient\n",
    "    \"\"\"\n",
    "    def __init__(self, adata, patient_col='patient_id', label_col='Response_3m'):\n",
    "        \"\"\"\n",
    "        Initialize the MIL dataset\n",
    "        \n",
    "        Parameters:\n",
    "        - adata: AnnData object with latent representations and patient information\n",
    "        - patient_col: Column name for patient identifiers\n",
    "        - label_col: Column name for patient response labels\n",
    "        \"\"\"\n",
    "        self.adata = adata\n",
    "        self.patient_col = patient_col\n",
    "        self.label_col = label_col\n",
    "        \n",
    "        # Get unique patients\n",
    "        self.patients = adata.obs[patient_col].unique()\n",
    "        \n",
    "        # Create a mapping of patient to label\n",
    "        self.patient_to_label = dict(zip(\n",
    "            adata.obs[patient_col], \n",
    "            adata.obs[label_col]\n",
    "        ))\n",
    "\n",
    "        #############################################\n",
    "        \n",
    "\n",
    "        if \"Sample_source\" in adata.obs.columns:\n",
    "            # Create one-hot encoding for Sample_source using unique values\n",
    "            sample_sources = list(adata.obs[\"Sample_source\"].unique())\n",
    "            self.patient_metadata = {}\n",
    "            \n",
    "            # Get Sample_source for each patient\n",
    "            patient_source_map = adata.obs.groupby(self.patient_col, observed=True)['Sample_source'].first().to_dict()\n",
    "            \n",
    "            # Create one-hot encoding for each patient\n",
    "            for patient in self.patients:\n",
    "                if patient in patient_source_map:\n",
    "                    source = patient_source_map[patient]\n",
    "                    one_hot = [1 if s == source else 0 for s in sample_sources]\n",
    "                    self.patient_metadata[patient] = one_hot\n",
    "            \n",
    "\n",
    "\n",
    "        ##############################################\n",
    "        \n",
    "        # Create patient bags\n",
    "        self.patient_bags = {}\n",
    "        self.patient_labels = {}\n",
    "        \n",
    "        for patient in self.patients:\n",
    "            # Get indices for this patient\n",
    "            indices = np.where(adata.obs[patient_col] == patient)[0]\n",
    "            \n",
    "            # Get features for this patient's cells\n",
    "            patient_data = adata.X[indices]\n",
    "            if isinstance(patient_data, np.ndarray) == False:\n",
    "                patient_data = patient_data.toarray()\n",
    "                \n",
    "            self.patient_bags[patient] = patient_data\n",
    "            \n",
    "            # Get label for this patient\n",
    "            # Assuming all cells from the same patient have the same label\n",
    "            label = self.patient_to_label[patient]\n",
    "            self.patient_labels[patient] = label\n",
    "        \n",
    "        # Convert patients to a list for indexing\n",
    "        self.patient_list = list(self.patients)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of bags (patients)\"\"\"\n",
    "        return len(self.patient_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a patient bag\n",
    "        \n",
    "        Returns:\n",
    "        - bag: Tensor of shape [num_instances, features] for the patient\n",
    "        - label: Label for the patient\n",
    "        - patient: Patient identifier\n",
    "        \"\"\"\n",
    "        patient = self.patient_list[idx]\n",
    "        bag = self.patient_bags[patient]\n",
    "        label = self.patient_labels[patient]\n",
    "        \n",
    "        # Convert to proper format\n",
    "        bag = torch.FloatTensor(bag)\n",
    "        \n",
    "        # Handle different label types\n",
    "        if isinstance(label, str):\n",
    "            # Map string labels to integers\n",
    "            \n",
    "            label_map = {\"NR\":0, \"OR\":1}\n",
    "            label = label_map[label]\n",
    "        \n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        if self.patient_metadata:\n",
    "            one_hot_sample_source = torch.tensor(self.patient_metadata[patient], dtype=torch.float)\n",
    "            return bag, label, patient, one_hot_sample_source\n",
    "        else:\n",
    "            return bag, label, patient\n",
    "\n",
    "# 2. Attention-based MIL model\n",
    "class AttentionMIL(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention-based Multiple Instance Learning model as used in scMILD\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_classes=2, hidden_dim=128, dropout=0.25, sample_source_dim=None):\n",
    "        \"\"\"\n",
    "        Initialize the MIL model\n",
    "        \n",
    "        Parameters:\n",
    "        - input_dim: Dimension of the input features (output of autoencoder)\n",
    "        - num_classes: Number of response classes\n",
    "        - hidden_dim: Dimension of hidden layer\n",
    "        - dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(AttentionMIL, self).__init__()\n",
    "\n",
    "        self.use_sample_source = sample_source_dim is not None\n",
    "        \n",
    "        # Feature extractor network\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            # nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            # nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        if self.use_sample_source:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(hidden_dim + sample_source_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, num_classes)\n",
    "            )\n",
    "        else:\n",
    "            # Classifier\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_dim, num_classes)\n",
    "            ) #nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x, sample_source=None, return_attention=False):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Parameters:\n",
    "        - x: Input bag of instances [batch_size, num_instances, features]\n",
    "        - return_attention: Whether to return attention weights\n",
    "        - sample_source: One-hot encoded\n",
    "        \n",
    "        Returns:\n",
    "        - logits: Class logits [batch_size, num_classes]\n",
    "        - attention_weights: Attention weights if return_attention=True\n",
    "        \"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        # x shape: [batch_size, num_instances, features]\n",
    "        batch_size = len(x)\n",
    "        \n",
    "        # Process each bag\n",
    "        all_logits = []\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            instances = x[i]  # [num_instances, features]\n",
    "            \n",
    "            # Extract features from each instance\n",
    "            instance_features = self.feature_extractor(instances)  # [num_instances, hidden_dim]\n",
    "            \n",
    "            # Calculate attention scores\n",
    "            attention_scores = self.attention(instance_features)  # [num_instances, 1]\n",
    "            attention_weights = F.softmax(attention_scores, dim=0)  # [num_instances, 1]\n",
    "            \n",
    "            # Calculate weighted average of instance features\n",
    "            weighted_features = torch.sum(\n",
    "                instance_features * attention_weights, dim=0\n",
    "            )  # [hidden_dim]\n",
    "            \n",
    "            \n",
    "\n",
    "            #########################################################\n",
    "            ########## Add sample source if provided #################\n",
    "            #########################################################\n",
    "            if self.use_sample_source and sample_source is not None:\n",
    "                # Print shape and example values for debugging\n",
    "                print(f\"Sample source shape: {sample_source.shape}\")\n",
    "                print(f\"Example sample source value: {sample_source[0]}\")\n",
    "            # get the sample source for this patient\n",
    "                sample_source_i = sample_source[i]\n",
    "\n",
    "                # Verify sample_source_i has the expected dimension\n",
    "                if len(sample_source_i) != 4:\n",
    "                    print(f\"Warning: sample_source dimension mismatch. Expected {4}, got {len(sample_source_i)}\")\n",
    "                \n",
    "                # weighted_features shape: [hidden_dim] (e.g. [128])\n",
    "                # sample_source_i shape: [4] \n",
    "                # Need to unsqueeze to match dimensions\n",
    "                weighted_features = weighted_features.unsqueeze(0)  # Shape: [1, hidden_dim]\n",
    "                sample_source_i = sample_source_i.unsqueeze(0)     # Shape: [1, 4]\n",
    "                # Concatenate along dim=1 since we're joining features\n",
    "                combined_features = torch.cat([weighted_features, sample_source_i], dim=1)  # Shape: [1, hidden_dim + 4]\n",
    "                combined_features = combined_features.squeeze(0)    # Back to [hidden_dim + 4] for classifier\n",
    "                logits = self.classifier(combined_features)\n",
    "            \n",
    "            else:\n",
    "                logits = self.classifier(weighted_features)\n",
    "        # Stack results\n",
    "        logits = torch.stack(all_logits)  # [batch_size, num_classes]\n",
    "        attention_weights = all_attention_weights  # List of [num_instances, 1]\n",
    "        \n",
    "        if return_attention:\n",
    "            return logits, attention_weights\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define a leave one out cross validation function\n",
    "def leave_one_out_cross_validation(adata, input_dim, num_classes=2, hidden_dim=128, sample_source_dim=4,\n",
    "                                  num_epochs=50, learning_rate=5e-4, weight_decay = 1e-2, \n",
    "                                  save_path='results'):\n",
    "    \"\"\"\n",
    "    Perform leave-one-out cross-validation for the MIL model\n",
    "    # Parameters:\n",
    "    - adata: AnnData object with latent representations\n",
    "    - input_dim: Input dimension (latent space dimension)\n",
    "    - num_classes: Number of response classes\n",
    "    - hidden_dim: Dimension of hidden layer\n",
    "    - num_epochs: Maximum number of training epochs\n",
    "    - learning_rate: Learning rate for optimizer\n",
    "    - weight_decay: L2 regularization strength\n",
    "\n",
    "    Returns:\n",
    "    - cv_results: Dictionary of cross-validation results\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # initiate wandb\n",
    "    # wandb.init(project=\"car-t-IP-MIL\", name=\"loocv-mil\",\n",
    "    #            config={\n",
    "    #                \"input_dim\": input_dim,\n",
    "    #                \"num_classes\": num_classes,\n",
    "    #                \"hidden_dim\": hidden_dim,\n",
    "    #                \"num_epochs\": num_epochs,\n",
    "    #                \"learning_rate\": learning_rate,\n",
    "    #                \"weight_decay\": weight_decay,\n",
    "    #                \"save_path\": save_path\n",
    "    #            })\n",
    "    \n",
    "    # create dataset\n",
    "    full_dataset = PatientBagDataset(adata)\n",
    "\n",
    "    # get all patients and their labels\n",
    "    patients = np.array(full_dataset.patient_list)\n",
    "    labels = np.array([full_dataset.patient_labels[p] for p in patients])\n",
    "\n",
    "    print(f\"Performing leave-one-out cross-validation for {len(patients)} patients...\")\n",
    "\n",
    "    cv_results = {\n",
    "        'fold_metrics': [], # store per-fold metrics for reference\n",
    "        'patient_predictions': {},\n",
    "        'attention_weights': {}\n",
    "    }\n",
    "\n",
    "    # Initialize accumulators for all predictions across folds\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "    all_prediction_probs = []\n",
    "    all_patient_ids = []\n",
    "\n",
    "    # wandb_patient_table = wandb.Table(columns=[\"patient_id\", \"true_label\", \"predicted_label\", \"predicted_label\", \"correct\"])\n",
    "\n",
    "    # all_metrics = []\n",
    "    # all_confusion_matrices = np.zeros((num_classes, num_classes))\n",
    "\n",
    "\n",
    "    # LOOCV loop\n",
    "    for i, test_patient in enumerate(patients):\n",
    "\n",
    "        print(f\"Fold {i+1}/{len(patients)} patients, testing on {test_patient}...\")\n",
    "        train_patients = np.array([p for p in patients if p != test_patient])\n",
    "        \n",
    "\n",
    "        # create train and test datasets\n",
    "        train_dataset = PatientBagDataset(adata.copy()[adata.obs['patient_id'].isin(train_patients)])\n",
    "        test_dataset = PatientBagDataset(adata.copy()[adata.obs['patient_id'] == test_patient])\n",
    "\n",
    "        # create data loaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        # create save path for this fold\n",
    "        fold_save_path = os.path.join(save_path, f'patient_{test_patient}')\n",
    "        os.makedirs(fold_save_path, exist_ok=True)\n",
    "\n",
    "        # train model\n",
    "        model = AttentionMIL(input_dim, num_classes, hidden_dim, sample_source_dim).to(device)\n",
    "\n",
    "        # use weight classes to address class imbalance\n",
    "        y = adata.obs.Response_3m.to_numpy()\n",
    "        y = np.where(y == \"NR\", 0,1)\n",
    "    \n",
    "        class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y), y=y)\n",
    "        class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "        \n",
    "        # Loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "    \n",
    "\n",
    "        # criterion = torch.nn.CrossEntropyLoss()\n",
    "        # optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "        \n",
    "        # Training setup\n",
    "        best_train_loss = float(\"inf\")\n",
    "        epochs_without_improvement = 0\n",
    "        patience = 8\n",
    "\n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': []\n",
    "            \n",
    "        }\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            for bags, batch_labels, _, one_hot_sample_source in train_loader:\n",
    "                bags = [bag.to(device) for bag in bags]\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                one_hot_sample_source = one_hot_sample_source.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                logits = model(bags, one_hot_sample_source)\n",
    "                loss = criterion(logits, batch_labels)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update metrics\n",
    "                train_loss += loss.item()\n",
    "                _, preds = torch.max(logits.data, 1)\n",
    "                train_total += batch_labels.size(0)\n",
    "                train_correct += (preds == batch_labels).sum().item()\n",
    "\n",
    "            train_loss /= len(train_loader)\n",
    "            train_acc = train_correct / train_total if train_total > 0 else 0\n",
    "\n",
    "            # record history\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "\n",
    "\n",
    "            # use training loss for scheduler and early stopping since we have no validation set\n",
    "            scheduler.step(train_loss)\n",
    "\n",
    "            # print progress every 20 epochs\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f'Patient {test_patient} - Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "\n",
    "            # log metrics\n",
    "            # wandb.log({\n",
    "            #     f\"patient_{test_patient}/epoch\": epoch + 1,\n",
    "            #     f\"patient_{test_patient}/train_loss\": train_loss,\n",
    "            #     f\"patient_{test_patient}/train_acc\": train_acc,\n",
    "            #     f\"patient_{test_patient}/learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            # })\n",
    "\n",
    "            # early stopping\n",
    "            if train_loss < best_train_loss:\n",
    "                best_train_loss = train_loss\n",
    "                epochs_without_improvement = 0\n",
    "\n",
    "                #save best model\n",
    "                torch.save(model.state_dict(), os.path.join(fold_save_path, 'best_model.pth'))\n",
    "\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(f'Early stopping after {epoch+1} epochs')\n",
    "                    break\n",
    "        \n",
    "        # load best model\n",
    "        model.load_state_dict(torch.load(os.path.join(fold_save_path, 'best_model.pth')))\n",
    "\n",
    "        # evaluate on test patient\n",
    "        model.eval()\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "        # test_preds = []\n",
    "        # test_labels = []\n",
    "        # test_probs = []\n",
    "        # patient_attentions = {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for bags, batch_labels, patient_ids, one_hot_sample_source in test_loader:\n",
    "                bags = [bag.to(device) for bag in bags]\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                one_hot_sample_source = one_hot_sample_source.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                logits, attn_weights = model(bags, one_hot_sample_source, return_attention=True)\n",
    "\n",
    "                # get predictions\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                _, preds = torch.max(logits, 1)\n",
    "\n",
    "                # convert to numpy\n",
    "                preds_np = preds.cpu().numpy()\n",
    "                labels_np = batch_labels.cpu().numpy()\n",
    "                probs_np = probs.cpu().numpy()\n",
    "\n",
    "                # Get the patient_id and store results\n",
    "                patient_id = patient_ids[0]\n",
    "                true_label = labels_np[0]\n",
    "                pred_label = preds_np[0]\n",
    "                pos_prob = probs_np[0, 1] if num_classes == 2 else None\n",
    "\n",
    "                # Add to accumulators for global metrics\n",
    "                all_true_labels.append(true_label)\n",
    "                all_predicted_labels.append(pred_label)\n",
    "                all_prediction_probs.append(pos_prob if num_classes ==2 else probs_np[0])\n",
    "                all_patient_ids.append(patient_id)\n",
    "\n",
    "                # Store patient-specific results\n",
    "                cv_results['patient_predictions'][patient_id] = {\n",
    "                    'true_label': true_label,\n",
    "                    'predicted_label': pred_label,\n",
    "                    'probabilities': probs_np[0].tolist(),\n",
    "                    'correct': (pred_label == true_label)\n",
    "                }\n",
    "                \n",
    "                    \n",
    "                \n",
    "\n",
    "                # Store attention weights\n",
    "                cv_results['attention_weights'][patient_id] = [w.cpu().numpy() for w in attn_weights]\n",
    "\n",
    "                # Add to wandb table\n",
    "                # wandb_patient_table.add_data(\n",
    "                #     patient_id, \n",
    "                #     int(true_label), \n",
    "                #     int(pred_label), \n",
    "                #     float(pos_prob) if num_classes == 2 else 'N/A',\n",
    "                #     bool(pred_label == true_label)\n",
    "                # )\n",
    "\n",
    "                \n",
    "        # Calculate per-fold metrics for individual patient (for monitoring only)\n",
    "        fold_correct = (preds_np[0] == labels_np[0])\n",
    "        fold_metrics = {\n",
    "            'patient_id': patient_id,\n",
    "            'fold': i,\n",
    "            'accuracy': 1.0 if fold_correct else 0.0,\n",
    "            'true_label': int(labels_np[0]),\n",
    "            'predicted_label': int(preds_np[0]),\n",
    "            'prob_positive': float(probs_np[0, 1]) if num_classes == 2 else None\n",
    "        }\n",
    "        cv_results['fold_metrics'].append(fold_metrics)\n",
    "        \n",
    "        # Log patient result to wandb\n",
    "        # wandb.log({\n",
    "        #     f\"patient_{patient_id}/true_label\": labels_np[0],\n",
    "        #     f\"patient_{patient_id}/predicted_label\": preds_np[0],\n",
    "        #     f\"patient_{patient_id}/prob_positive\": probs_np[0, 1] if num_classes == 2 else None,\n",
    "        #     f\"patient_{patient_id}/correct\": fold_correct\n",
    "        # })\n",
    "\n",
    "    # Convert accumulators to numpy arrays\n",
    "    all_true_labels = np.array(all_true_labels)\n",
    "    all_predicted_labels = np.array(all_predicted_labels)\n",
    "    all_prediction_probs = np.array(all_prediction_probs)\n",
    "    all_patient_ids = np.array(all_patient_ids)\n",
    "    \n",
    "    # Calculate overall metrics only once using ALL predictions\n",
    "    overall_accuracy = accuracy_score(all_true_labels, all_predicted_labels)\n",
    "    \n",
    "    # For binary classification\n",
    "    if num_classes == 2:\n",
    "        overall_precision = precision_score(all_true_labels, all_predicted_labels)\n",
    "        overall_recall = recall_score(all_true_labels, all_predicted_labels)\n",
    "        overall_f1 = f1_score(all_true_labels, all_predicted_labels)\n",
    "        overall_auc = roc_auc_score(all_true_labels, all_prediction_probs)\n",
    "    else:\n",
    "        # For multiclass classification\n",
    "        overall_precision = precision_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "        overall_recall = recall_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "        overall_f1 = f1_score(all_true_labels, all_predicted_labels, average='weighted')\n",
    "        # For multiclass AUC, we'd need to calculate it differently (beyond scope here)\n",
    "        overall_auc = 0.5  \n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "    \n",
    "    # Calculate class-specific metrics\n",
    "    class_metrics = {}\n",
    "    for c in range(num_classes):\n",
    "        true_positives = np.sum((all_true_labels == c) & (all_predicted_labels == c))\n",
    "        actual_positives = np.sum(all_true_labels == c)\n",
    "        predicted_positives = np.sum(all_predicted_labels == c)\n",
    "        \n",
    "        class_metrics[f'class_{c}'] = {\n",
    "            'precision': true_positives / predicted_positives if predicted_positives > 0 else 0,\n",
    "            'recall': true_positives / actual_positives if actual_positives > 0 else 0,\n",
    "            'count': int(actual_positives)\n",
    "        }\n",
    "    \n",
    "    # Store final metrics in results\n",
    "    cv_results['overall_metrics'] = {\n",
    "        'accuracy': float(overall_accuracy),\n",
    "        'precision': float(overall_precision),\n",
    "        'recall': float(overall_recall),\n",
    "        'f1': float(overall_f1),\n",
    "        'auc': float(overall_auc),\n",
    "        'confusion_matrix': conf_matrix.tolist(),\n",
    "        'class_metrics': class_metrics\n",
    "    }\n",
    "    \n",
    "    # Log final results to wandb\n",
    "    # wandb.log({\n",
    "    #     \"overall_accuracy\": overall_accuracy,\n",
    "    #     \"overall_precision\": overall_precision,\n",
    "    #     \"overall_recall\": overall_recall,\n",
    "    #     \"overall_f1\": overall_f1,\n",
    "    #     \"overall_auc\": overall_auc,\n",
    "    #     \"patient_results\": wandb_patient_table\n",
    "    # })\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\n===== LOOCV Final Results =====\")\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.4f} ({np.sum(all_predicted_labels == all_true_labels)}/{len(all_true_labels)} patients correct)\")\n",
    "    print(f\"Overall Precision: {overall_precision:.4f}\")\n",
    "    print(f\"Overall Recall: {overall_recall:.4f}\")\n",
    "    print(f\"Overall F1 Score: {overall_f1:.4f}\")\n",
    "    print(f\"Overall AUC: {overall_auc:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"\\nClass-specific Metrics:\")\n",
    "    for class_name, metrics in class_metrics.items():\n",
    "        print(f\"{class_name}: Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, Count={metrics['count']}\")\n",
    "    \n",
    "    # Save results to file\n",
    "    results_path = os.path.join(save_path, 'loocv_results.pkl')\n",
    "    with open(results_path, 'wb') as f:\n",
    "        import pickle\n",
    "        pickle.dump(cv_results, f)\n",
    "    \n",
    "    return cv_results\n",
    "       \n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c26a3205-c9d4-4d17-b881-42f981b360aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def run_pipeline_loocv(input_file, output_dir='results',\n",
    "                       latent_dim=64, num_epochs_ae=200,\n",
    "                       num_epochs=50, num_classes=2,\n",
    "                       hidden_dim=128, sample_source_dim=4,\n",
    "                       project_name=\"car-t-response\"):\n",
    "    \"\"\"run complete pipeline with leave one out cross validation\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file: path to input file\n",
    "    - output_dir: directory to save results\n",
    "    - latent_dim: dimension of latent space\n",
    "    - num_epochs_ae: number of epochs for autoencoder\n",
    "    - num_epoch_mil: number of epochs for MIL\n",
    "    - num_classes: number of classes\n",
    "    - hidden_dim: dimension of hidden layer\n",
    "\n",
    "    Returns:\n",
    "    - dict of results and models\n",
    "    \"\"\"\n",
    "\n",
    "    # config = {\n",
    "    #     \"input_file\": input_file,\n",
    "    #     \"output_dir\": output_dir,\n",
    "    #     \"latent_dim\": latent_dim,\n",
    "    #     \"num_epochs_ae\": num_epochs_ae,\n",
    "    #     \"num_epochs_mil\": num_epochs,\n",
    "    #     \"num_classes\": num_classes,\n",
    "    #     \"hidden_dim\": hidden_dim,\n",
    "    #     \"cv_method\": \"leave-one-out\"\n",
    "    # }\n",
    "\n",
    "    # wandb.init(project=project_name, config=config)\n",
    "\n",
    "    # Create output directories\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    result_dir = os.path.join(output_dir, f\"run_{timestamp}\")\n",
    "    ae_dir = os.path.join(result_dir, \"autoencoder\")\n",
    "    mil_dir = os.path.join(result_dir, \"mil\")\n",
    "    \n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    os.makedirs(ae_dir, exist_ok=True)\n",
    "    os.makedirs(mil_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # Step 1: Load and explore data\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 1: LOADING AND EXPLORING DATA\")\n",
    "    print(\"=\"*80)\n",
    "    adata = load_and_explore_data(input_file)\n",
    "\n",
    "    # wandb.config.update({\n",
    "    #     \"cells\": adata.n_obs,\n",
    "    #     \"TFs\": adata.n_vars,\n",
    "    #     \"patients\": adata.obs[\"patient_id\"].nunique()\n",
    "    # })\n",
    "\n",
    "    # if \"Response_3m\" in adata.obs.columns:\n",
    "    #     wandb.config.update({\n",
    "    #         \"Response_distribution\": dict(adata.obs[\"Response_3m\"].value_counts())\n",
    "    #     })\n",
    "\n",
    "    # step 2: train autoencoder\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 2: TRAINING AUTOENCODER\")\n",
    "    print(\"=\"*80)\n",
    "    train_loader, val_loader, test_loader, input_dim = preprocess_data(adata)\n",
    "\n",
    "    # Step 3:train autoencoder\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 3: TRAINING AUTOENCODER\")\n",
    "    print(\"=\"*80)\n",
    "    model, train_losses, val_losses = train_autoencoder(\n",
    "            train_loader, val_loader, input_dim, latent_dim, num_epochs_ae, save_path=ae_dir\n",
    "        )\n",
    "    adata_latent, test_loss = evaluate_autoencoder(\n",
    "        model, test_loader, adata, adata.var_names.tolist(), save_path=ae_dir\n",
    "    )\n",
    "    \n",
    "    # Save latent representations\n",
    "    latent_file = os.path.join(ae_dir, \"latent_representation.h5ad\")\n",
    "    adata_latent.write(latent_file)\n",
    "\n",
    "    # Step 4: Run LOOCV\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STEP 4: RUNNING LEAVE-ONE-OUT CROSS-VALIDATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Check if we have response information\n",
    "    if 'Response_3m' not in adata_latent.obs.columns:\n",
    "        print(\"ERROR: 'response' column not found in the data. Cannot proceed with MIL.\")\n",
    "        # wandb.finish()\n",
    "        return None\n",
    "    \n",
    "    # Remove patients with NaN responses\n",
    "    patients_with_missing = adata_latent.obs[adata_latent.obs['Response_3m'].isna()]['patient_id'].unique()\n",
    "    if len(patients_with_missing) > 0:\n",
    "        print(f\"Removing {len(patients_with_missing)} patients with missing responses\")\n",
    "        adata_latent = adata_latent[~adata_latent.obs['patient_id'].isin(patients_with_missing)].copy()\n",
    "        \n",
    "\n",
    "    cv_results = leave_one_out_cross_validation(\n",
    "        adata_latent, \n",
    "        input_dim = latent_dim,\n",
    "        num_classes = num_classes, \n",
    "        hidden_dim = hidden_dim,\n",
    "        sample_source_dim = sample_source_dim,\n",
    "        num_epochs = num_epochs,\n",
    "        save_path = mil_dir\n",
    "    )\n",
    "        \n",
    "\n",
    "\n",
    "    # wandb.finish()\n",
    "\n",
    "    print(f\"Pipeline completed successfully! Results saved to {result_dir}\")\n",
    "\n",
    "    return {\n",
    "        'adata': adata,\n",
    "        'autoencoder': model,\n",
    "        'latent_data': adata_latent,\n",
    "        'mil_results': cv_results,\n",
    "        'results_dir': result_dir\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98481c96-ffcd-4ddf-84b1-ef9aefd1b1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: LOADING AND EXPLORING DATA\n",
      "================================================================================\n",
      "Loading SCENIC AUC matrix data...\n",
      "Total cells: 35937\n",
      "Number of TFs: 154\n",
      "Number of patients: 64\n",
      "Response distribution:\n",
      "Response_3m\n",
      "OR    18920\n",
      "NR    17017\n",
      "Name: count, dtype: int64\n",
      "AUC matrix shape: (35937, 154)\n",
      "AUC value range: [-1.0, 0.770928626247122]\n",
      "AUC mean value: -0.9312020449924552\n",
      "\n",
      "================================================================================\n",
      "STEP 2: TRAINING AUTOENCODER\n",
      "================================================================================\n",
      "Preprocessing data for autoencoder training...\n",
      "Training set: 24470 cells\n",
      "Validation set: 4200 cells\n",
      "Test set: 7267 cells\n",
      "\n",
      "================================================================================\n",
      "STEP 3: TRAINING AUTOENCODER\n",
      "================================================================================\n",
      "Training on: cuda\n",
      "Starting training for 150 epochs...\n",
      "Epoch [1/150], Train Loss: 0.289396, Val Loss: 0.026377\n",
      "Epoch [2/150], Train Loss: 0.023972, Val Loss: 0.008244\n",
      "Epoch [3/150], Train Loss: 0.011571, Val Loss: 0.005688\n",
      "Epoch [4/150], Train Loss: 0.008333, Val Loss: 0.004491\n",
      "Epoch [5/150], Train Loss: 0.006747, Val Loss: 0.003925\n",
      "Epoch [6/150], Train Loss: 0.005937, Val Loss: 0.003630\n",
      "Epoch [7/150], Train Loss: 0.005428, Val Loss: 0.003435\n",
      "Epoch [8/150], Train Loss: 0.005073, Val Loss: 0.003345\n",
      "Epoch [9/150], Train Loss: 0.004873, Val Loss: 0.003314\n",
      "Epoch [10/150], Train Loss: 0.004673, Val Loss: 0.003162\n",
      "Epoch [11/150], Train Loss: 0.004520, Val Loss: 0.003164\n",
      "Epoch [12/150], Train Loss: 0.004433, Val Loss: 0.003089\n",
      "Epoch [13/150], Train Loss: 0.004364, Val Loss: 0.003087\n",
      "Epoch [14/150], Train Loss: 0.004321, Val Loss: 0.003022\n",
      "Epoch [15/150], Train Loss: 0.004268, Val Loss: 0.003015\n",
      "Epoch [16/150], Train Loss: 0.004208, Val Loss: 0.002995\n",
      "Epoch [17/150], Train Loss: 0.004182, Val Loss: 0.003000\n",
      "Epoch [18/150], Train Loss: 0.004210, Val Loss: 0.002998\n",
      "Epoch [19/150], Train Loss: 0.004157, Val Loss: 0.002984\n",
      "Epoch [20/150], Train Loss: 0.004160, Val Loss: 0.002983\n",
      "Epoch [21/150], Train Loss: 0.004182, Val Loss: 0.002983\n",
      "Epoch [22/150], Train Loss: 0.004165, Val Loss: 0.002983\n",
      "Epoch [23/150], Train Loss: 0.004183, Val Loss: 0.002989\n",
      "Epoch [24/150], Train Loss: 0.004175, Val Loss: 0.002969\n",
      "Epoch [25/150], Train Loss: 0.004139, Val Loss: 0.002969\n",
      "Epoch [26/150], Train Loss: 0.004088, Val Loss: 0.002955\n",
      "Epoch [27/150], Train Loss: 0.004079, Val Loss: 0.002882\n",
      "Epoch [28/150], Train Loss: 0.004004, Val Loss: 0.002868\n",
      "Epoch [29/150], Train Loss: 0.003915, Val Loss: 0.002751\n",
      "Epoch [30/150], Train Loss: 0.003842, Val Loss: 0.002746\n",
      "Epoch [31/150], Train Loss: 0.003747, Val Loss: 0.002703\n",
      "Epoch [32/150], Train Loss: 0.003500, Val Loss: 0.002599\n",
      "Epoch [33/150], Train Loss: 0.003320, Val Loss: 0.002473\n",
      "Epoch [34/150], Train Loss: 0.003195, Val Loss: 0.002265\n",
      "Epoch [35/150], Train Loss: 0.003018, Val Loss: 0.002240\n",
      "Epoch [36/150], Train Loss: 0.002831, Val Loss: 0.002153\n",
      "Epoch [37/150], Train Loss: 0.002704, Val Loss: 0.002071\n",
      "Epoch [38/150], Train Loss: 0.002620, Val Loss: 0.002046\n",
      "Epoch [39/150], Train Loss: 0.002533, Val Loss: 0.002004\n",
      "Epoch [40/150], Train Loss: 0.002477, Val Loss: 0.001935\n",
      "Epoch [41/150], Train Loss: 0.002420, Val Loss: 0.001928\n",
      "Epoch [42/150], Train Loss: 0.002376, Val Loss: 0.001895\n",
      "Epoch [43/150], Train Loss: 0.002339, Val Loss: 0.001864\n",
      "Epoch [44/150], Train Loss: 0.002292, Val Loss: 0.001819\n",
      "Epoch [45/150], Train Loss: 0.002249, Val Loss: 0.001834\n",
      "Epoch [46/150], Train Loss: 0.002237, Val Loss: 0.001800\n",
      "Epoch [47/150], Train Loss: 0.002204, Val Loss: 0.001757\n",
      "Epoch [48/150], Train Loss: 0.002188, Val Loss: 0.001712\n",
      "Epoch [49/150], Train Loss: 0.002153, Val Loss: 0.001678\n",
      "Epoch [50/150], Train Loss: 0.002136, Val Loss: 0.001661\n",
      "Epoch [51/150], Train Loss: 0.002113, Val Loss: 0.001672\n",
      "Epoch [52/150], Train Loss: 0.002093, Val Loss: 0.001642\n",
      "Epoch [53/150], Train Loss: 0.002080, Val Loss: 0.001624\n",
      "Epoch [54/150], Train Loss: 0.002066, Val Loss: 0.001625\n",
      "Epoch [55/150], Train Loss: 0.002054, Val Loss: 0.001615\n",
      "Epoch [56/150], Train Loss: 0.002048, Val Loss: 0.001598\n",
      "Epoch [57/150], Train Loss: 0.002043, Val Loss: 0.001609\n",
      "Epoch [58/150], Train Loss: 0.002038, Val Loss: 0.001600\n",
      "Epoch [59/150], Train Loss: 0.002037, Val Loss: 0.001596\n",
      "Epoch [60/150], Train Loss: 0.002042, Val Loss: 0.001599\n",
      "Epoch [61/150], Train Loss: 0.002037, Val Loss: 0.001599\n",
      "Epoch [62/150], Train Loss: 0.002041, Val Loss: 0.001598\n",
      "Epoch [63/150], Train Loss: 0.002032, Val Loss: 0.001596\n",
      "Epoch [64/150], Train Loss: 0.002029, Val Loss: 0.001595\n",
      "Epoch [65/150], Train Loss: 0.002037, Val Loss: 0.001609\n",
      "Epoch [66/150], Train Loss: 0.002018, Val Loss: 0.001585\n",
      "Epoch [67/150], Train Loss: 0.002026, Val Loss: 0.001585\n",
      "Epoch [68/150], Train Loss: 0.002020, Val Loss: 0.001582\n",
      "Epoch [69/150], Train Loss: 0.002018, Val Loss: 0.001576\n",
      "Epoch [70/150], Train Loss: 0.002007, Val Loss: 0.001575\n",
      "Epoch [71/150], Train Loss: 0.002003, Val Loss: 0.001567\n",
      "Epoch [72/150], Train Loss: 0.001988, Val Loss: 0.001557\n",
      "Epoch [73/150], Train Loss: 0.001958, Val Loss: 0.001522\n",
      "Epoch [74/150], Train Loss: 0.001943, Val Loss: 0.001519\n",
      "Epoch [75/150], Train Loss: 0.001938, Val Loss: 0.001497\n",
      "Epoch [76/150], Train Loss: 0.001913, Val Loss: 0.001475\n",
      "Epoch [77/150], Train Loss: 0.001899, Val Loss: 0.001508\n",
      "Epoch [78/150], Train Loss: 0.001893, Val Loss: 0.001515\n",
      "Epoch [79/150], Train Loss: 0.001868, Val Loss: 0.001414\n",
      "Epoch [80/150], Train Loss: 0.001851, Val Loss: 0.001460\n",
      "Epoch [81/150], Train Loss: 0.001838, Val Loss: 0.001455\n",
      "Epoch [82/150], Train Loss: 0.001823, Val Loss: 0.001394\n",
      "Epoch [83/150], Train Loss: 0.001808, Val Loss: 0.001407\n",
      "Epoch [84/150], Train Loss: 0.001793, Val Loss: 0.001393\n",
      "Epoch [85/150], Train Loss: 0.001786, Val Loss: 0.001372\n",
      "Epoch [86/150], Train Loss: 0.001770, Val Loss: 0.001396\n",
      "Epoch [87/150], Train Loss: 0.001763, Val Loss: 0.001395\n",
      "Epoch [88/150], Train Loss: 0.001754, Val Loss: 0.001363\n",
      "Epoch [89/150], Train Loss: 0.001745, Val Loss: 0.001343\n",
      "Epoch [90/150], Train Loss: 0.001731, Val Loss: 0.001347\n",
      "Epoch [91/150], Train Loss: 0.001720, Val Loss: 0.001342\n",
      "Epoch [92/150], Train Loss: 0.001711, Val Loss: 0.001341\n",
      "Epoch [93/150], Train Loss: 0.001707, Val Loss: 0.001325\n",
      "Epoch [94/150], Train Loss: 0.001708, Val Loss: 0.001330\n",
      "Epoch [95/150], Train Loss: 0.001697, Val Loss: 0.001327\n",
      "Epoch [96/150], Train Loss: 0.001691, Val Loss: 0.001308\n",
      "Epoch [97/150], Train Loss: 0.001692, Val Loss: 0.001315\n",
      "Epoch [98/150], Train Loss: 0.001691, Val Loss: 0.001309\n",
      "Epoch [99/150], Train Loss: 0.001684, Val Loss: 0.001308\n",
      "Epoch [100/150], Train Loss: 0.001688, Val Loss: 0.001308\n",
      "Epoch [101/150], Train Loss: 0.001680, Val Loss: 0.001308\n",
      "Epoch [102/150], Train Loss: 0.001682, Val Loss: 0.001309\n",
      "Epoch [103/150], Train Loss: 0.001684, Val Loss: 0.001307\n",
      "Epoch [104/150], Train Loss: 0.001683, Val Loss: 0.001307\n",
      "Epoch [105/150], Train Loss: 0.001685, Val Loss: 0.001309\n",
      "Epoch [106/150], Train Loss: 0.001685, Val Loss: 0.001316\n",
      "Epoch [107/150], Train Loss: 0.001690, Val Loss: 0.001311\n",
      "Epoch [108/150], Train Loss: 0.001683, Val Loss: 0.001320\n",
      "Epoch [109/150], Train Loss: 0.001687, Val Loss: 0.001319\n",
      "Epoch [110/150], Train Loss: 0.001681, Val Loss: 0.001321\n",
      "Epoch [111/150], Train Loss: 0.001680, Val Loss: 0.001311\n",
      "Epoch [112/150], Train Loss: 0.001669, Val Loss: 0.001299\n",
      "Epoch [113/150], Train Loss: 0.001666, Val Loss: 0.001323\n",
      "Epoch [114/150], Train Loss: 0.001659, Val Loss: 0.001268\n",
      "Epoch [115/150], Train Loss: 0.001643, Val Loss: 0.001263\n",
      "Epoch [116/150], Train Loss: 0.001646, Val Loss: 0.001297\n",
      "Epoch [117/150], Train Loss: 0.001629, Val Loss: 0.001290\n",
      "Epoch [118/150], Train Loss: 0.001617, Val Loss: 0.001257\n",
      "Epoch [119/150], Train Loss: 0.001606, Val Loss: 0.001245\n",
      "Epoch [120/150], Train Loss: 0.001591, Val Loss: 0.001223\n",
      "Epoch [121/150], Train Loss: 0.001578, Val Loss: 0.001250\n",
      "Epoch [122/150], Train Loss: 0.001561, Val Loss: 0.001200\n",
      "Epoch [123/150], Train Loss: 0.001560, Val Loss: 0.001213\n",
      "Epoch [124/150], Train Loss: 0.001532, Val Loss: 0.001205\n",
      "Epoch [125/150], Train Loss: 0.001520, Val Loss: 0.001168\n",
      "Epoch [126/150], Train Loss: 0.001518, Val Loss: 0.001173\n",
      "Epoch [127/150], Train Loss: 0.001509, Val Loss: 0.001196\n",
      "Epoch [128/150], Train Loss: 0.001494, Val Loss: 0.001172\n",
      "Epoch [129/150], Train Loss: 0.001482, Val Loss: 0.001139\n",
      "Epoch [130/150], Train Loss: 0.001478, Val Loss: 0.001146\n",
      "Epoch [131/150], Train Loss: 0.001471, Val Loss: 0.001134\n",
      "Epoch [132/150], Train Loss: 0.001466, Val Loss: 0.001131\n",
      "Epoch [133/150], Train Loss: 0.001456, Val Loss: 0.001134\n",
      "Epoch [134/150], Train Loss: 0.001446, Val Loss: 0.001126\n",
      "Epoch [135/150], Train Loss: 0.001446, Val Loss: 0.001113\n",
      "Epoch [136/150], Train Loss: 0.001450, Val Loss: 0.001116\n",
      "Epoch [137/150], Train Loss: 0.001442, Val Loss: 0.001111\n",
      "Epoch [138/150], Train Loss: 0.001434, Val Loss: 0.001109\n",
      "Epoch [139/150], Train Loss: 0.001439, Val Loss: 0.001104\n",
      "Epoch [140/150], Train Loss: 0.001439, Val Loss: 0.001109\n",
      "Epoch [141/150], Train Loss: 0.001434, Val Loss: 0.001109\n",
      "Epoch [142/150], Train Loss: 0.001437, Val Loss: 0.001107\n",
      "Epoch [143/150], Train Loss: 0.001434, Val Loss: 0.001105\n",
      "Epoch [144/150], Train Loss: 0.001437, Val Loss: 0.001108\n",
      "Epoch [145/150], Train Loss: 0.001441, Val Loss: 0.001106\n",
      "Epoch [146/150], Train Loss: 0.001443, Val Loss: 0.001109\n",
      "Epoch [147/150], Train Loss: 0.001442, Val Loss: 0.001108\n",
      "Epoch [148/150], Train Loss: 0.001442, Val Loss: 0.001119\n",
      "Epoch [149/150], Train Loss: 0.001441, Val Loss: 0.001113\n",
      "Early stopping after 149 epochs\n",
      "Test Loss: 0.001028\n",
      "\n",
      "================================================================================\n",
      "STEP 4: RUNNING LEAVE-ONE-OUT CROSS-VALIDATION\n",
      "================================================================================\n",
      "Using device: cuda\n",
      "Performing leave-one-out cross-validation for 64 patients...\n",
      "Fold 1/64 patients, testing on Pt237...\n",
      "Patient Pt237 - Epoch 20/60, Train Loss: 0.5868, Train Acc: 0.6825\n",
      "Patient Pt237 - Epoch 40/60, Train Loss: 0.4923, Train Acc: 0.8254\n",
      "Patient Pt237 - Epoch 60/60, Train Loss: 0.3998, Train Acc: 0.8254\n",
      "Fold 2/64 patients, testing on Pt245...\n",
      "Patient Pt245 - Epoch 20/60, Train Loss: 0.5760, Train Acc: 0.6984\n",
      "Patient Pt245 - Epoch 40/60, Train Loss: 0.4742, Train Acc: 0.8095\n",
      "Early stopping after 58 epochs\n",
      "Fold 3/64 patients, testing on Pt253...\n",
      "Patient Pt253 - Epoch 20/60, Train Loss: 0.5649, Train Acc: 0.7460\n",
      "Early stopping after 35 epochs\n",
      "Fold 4/64 patients, testing on Pt263...\n",
      "Patient Pt263 - Epoch 20/60, Train Loss: 0.5087, Train Acc: 0.8254\n",
      "Patient Pt263 - Epoch 40/60, Train Loss: 0.4729, Train Acc: 0.8095\n",
      "Patient Pt263 - Epoch 60/60, Train Loss: 0.4299, Train Acc: 0.7937\n",
      "Fold 5/64 patients, testing on Pt276...\n",
      "Patient Pt276 - Epoch 20/60, Train Loss: 0.5831, Train Acc: 0.7460\n",
      "Patient Pt276 - Epoch 40/60, Train Loss: 0.5185, Train Acc: 0.7460\n",
      "Patient Pt276 - Epoch 60/60, Train Loss: 0.4399, Train Acc: 0.8095\n",
      "Fold 6/64 patients, testing on Pt282...\n",
      "Patient Pt282 - Epoch 20/60, Train Loss: 0.5573, Train Acc: 0.7460\n",
      "Patient Pt282 - Epoch 40/60, Train Loss: 0.5111, Train Acc: 0.7937\n",
      "Patient Pt282 - Epoch 60/60, Train Loss: 0.5220, Train Acc: 0.7460\n",
      "Fold 7/64 patients, testing on Pt375...\n",
      "Patient Pt375 - Epoch 20/60, Train Loss: 0.5690, Train Acc: 0.7302\n",
      "Patient Pt375 - Epoch 40/60, Train Loss: 0.5204, Train Acc: 0.7460\n",
      "Patient Pt375 - Epoch 60/60, Train Loss: 0.4256, Train Acc: 0.7778\n",
      "Fold 8/64 patients, testing on 01...\n",
      "Patient 01 - Epoch 20/60, Train Loss: 0.5583, Train Acc: 0.7619\n",
      "Patient 01 - Epoch 40/60, Train Loss: 0.4895, Train Acc: 0.7619\n",
      "Patient 01 - Epoch 60/60, Train Loss: 0.4425, Train Acc: 0.8413\n",
      "Fold 9/64 patients, testing on 02...\n",
      "Patient 02 - Epoch 20/60, Train Loss: 0.5426, Train Acc: 0.7937\n",
      "Patient 02 - Epoch 40/60, Train Loss: 0.4639, Train Acc: 0.8254\n",
      "Patient 02 - Epoch 60/60, Train Loss: 0.3967, Train Acc: 0.8254\n",
      "Fold 10/64 patients, testing on 03...\n",
      "Patient 03 - Epoch 20/60, Train Loss: 0.5570, Train Acc: 0.7460\n",
      "Patient 03 - Epoch 40/60, Train Loss: 0.4934, Train Acc: 0.7619\n",
      "Patient 03 - Epoch 60/60, Train Loss: 0.4235, Train Acc: 0.8254\n",
      "Fold 11/64 patients, testing on 04...\n",
      "Patient 04 - Epoch 20/60, Train Loss: 0.5748, Train Acc: 0.7778\n",
      "Patient 04 - Epoch 40/60, Train Loss: 0.5050, Train Acc: 0.7937\n",
      "Patient 04 - Epoch 60/60, Train Loss: 0.3957, Train Acc: 0.8254\n",
      "Fold 12/64 patients, testing on 05...\n",
      "Patient 05 - Epoch 20/60, Train Loss: 0.5751, Train Acc: 0.6508\n",
      "Patient 05 - Epoch 40/60, Train Loss: 0.5387, Train Acc: 0.7460\n",
      "Patient 05 - Epoch 60/60, Train Loss: 0.4137, Train Acc: 0.8571\n",
      "Fold 13/64 patients, testing on 07...\n",
      "Patient 07 - Epoch 20/60, Train Loss: 0.5375, Train Acc: 0.7778\n",
      "Patient 07 - Epoch 40/60, Train Loss: 0.4600, Train Acc: 0.8413\n",
      "Early stopping after 50 epochs\n",
      "Fold 14/64 patients, testing on 08...\n",
      "Patient 08 - Epoch 20/60, Train Loss: 0.5876, Train Acc: 0.7460\n",
      "Patient 08 - Epoch 40/60, Train Loss: 0.4787, Train Acc: 0.8095\n",
      "Patient 08 - Epoch 60/60, Train Loss: 0.4145, Train Acc: 0.8254\n",
      "Fold 15/64 patients, testing on 09...\n",
      "Patient 09 - Epoch 20/60, Train Loss: 0.5947, Train Acc: 0.7143\n",
      "Patient 09 - Epoch 40/60, Train Loss: 0.4436, Train Acc: 0.7937\n",
      "Patient 09 - Epoch 60/60, Train Loss: 0.3754, Train Acc: 0.7937\n",
      "Fold 16/64 patients, testing on 10...\n",
      "Patient 10 - Epoch 20/60, Train Loss: 0.6000, Train Acc: 0.6984\n",
      "Patient 10 - Epoch 40/60, Train Loss: 0.4822, Train Acc: 0.7937\n",
      "Patient 10 - Epoch 60/60, Train Loss: 0.4676, Train Acc: 0.7937\n",
      "Fold 17/64 patients, testing on 11...\n",
      "Patient 11 - Epoch 20/60, Train Loss: 0.5508, Train Acc: 0.7460\n",
      "Patient 11 - Epoch 40/60, Train Loss: 0.5102, Train Acc: 0.7619\n",
      "Patient 11 - Epoch 60/60, Train Loss: 0.4140, Train Acc: 0.8254\n",
      "Fold 18/64 patients, testing on 12...\n",
      "Patient 12 - Epoch 20/60, Train Loss: 0.5806, Train Acc: 0.7460\n",
      "Patient 12 - Epoch 40/60, Train Loss: 0.5124, Train Acc: 0.7143\n",
      "Early stopping after 44 epochs\n",
      "Fold 19/64 patients, testing on 13...\n",
      "Patient 13 - Epoch 20/60, Train Loss: 0.5729, Train Acc: 0.7460\n",
      "Patient 13 - Epoch 40/60, Train Loss: 0.5376, Train Acc: 0.7302\n",
      "Patient 13 - Epoch 60/60, Train Loss: 0.4235, Train Acc: 0.8571\n",
      "Fold 20/64 patients, testing on 14...\n",
      "Patient 14 - Epoch 20/60, Train Loss: 0.5977, Train Acc: 0.6825\n",
      "Patient 14 - Epoch 40/60, Train Loss: 0.4660, Train Acc: 0.7778\n",
      "Patient 14 - Epoch 60/60, Train Loss: 0.4384, Train Acc: 0.8095\n",
      "Fold 21/64 patients, testing on 15...\n",
      "Patient 15 - Epoch 20/60, Train Loss: 0.5933, Train Acc: 0.6825\n",
      "Patient 15 - Epoch 40/60, Train Loss: 0.4749, Train Acc: 0.7778\n",
      "Patient 15 - Epoch 60/60, Train Loss: 0.4659, Train Acc: 0.8254\n",
      "Fold 22/64 patients, testing on 16...\n",
      "Patient 16 - Epoch 20/60, Train Loss: 0.5531, Train Acc: 0.7143\n",
      "Patient 16 - Epoch 40/60, Train Loss: 0.4910, Train Acc: 0.7302\n",
      "Patient 16 - Epoch 60/60, Train Loss: 0.4215, Train Acc: 0.7937\n",
      "Fold 23/64 patients, testing on 17...\n",
      "Patient 17 - Epoch 20/60, Train Loss: 0.5952, Train Acc: 0.7143\n",
      "Patient 17 - Epoch 40/60, Train Loss: 0.4533, Train Acc: 0.8095\n",
      "Patient 17 - Epoch 60/60, Train Loss: 0.4187, Train Acc: 0.8571\n",
      "Fold 24/64 patients, testing on 18...\n",
      "Patient 18 - Epoch 20/60, Train Loss: 0.5895, Train Acc: 0.6984\n",
      "Patient 18 - Epoch 40/60, Train Loss: 0.5066, Train Acc: 0.8254\n",
      "Patient 18 - Epoch 60/60, Train Loss: 0.4284, Train Acc: 0.8254\n",
      "Fold 25/64 patients, testing on 19...\n",
      "Patient 19 - Epoch 20/60, Train Loss: 0.5993, Train Acc: 0.7460\n",
      "Patient 19 - Epoch 40/60, Train Loss: 0.5022, Train Acc: 0.7778\n",
      "Patient 19 - Epoch 60/60, Train Loss: 0.4667, Train Acc: 0.8254\n",
      "Fold 26/64 patients, testing on ac01...\n",
      "Patient ac01 - Epoch 20/60, Train Loss: 0.5365, Train Acc: 0.7778\n",
      "Patient ac01 - Epoch 40/60, Train Loss: 0.4493, Train Acc: 0.8254\n",
      "Patient ac01 - Epoch 60/60, Train Loss: 0.4129, Train Acc: 0.8730\n",
      "Fold 27/64 patients, testing on ac02...\n",
      "Patient ac02 - Epoch 20/60, Train Loss: 0.5753, Train Acc: 0.6984\n",
      "Patient ac02 - Epoch 40/60, Train Loss: 0.4909, Train Acc: 0.7619\n",
      "Patient ac02 - Epoch 60/60, Train Loss: 0.4173, Train Acc: 0.8730\n",
      "Fold 28/64 patients, testing on ac03...\n",
      "Patient ac03 - Epoch 20/60, Train Loss: 0.5945, Train Acc: 0.6667\n",
      "Patient ac03 - Epoch 40/60, Train Loss: 0.5161, Train Acc: 0.7619\n",
      "Patient ac03 - Epoch 60/60, Train Loss: 0.4467, Train Acc: 0.8254\n",
      "Fold 29/64 patients, testing on ac04...\n",
      "Patient ac04 - Epoch 20/60, Train Loss: 0.5961, Train Acc: 0.7302\n",
      "Patient ac04 - Epoch 40/60, Train Loss: 0.5270, Train Acc: 0.7619\n",
      "Patient ac04 - Epoch 60/60, Train Loss: 0.4387, Train Acc: 0.7937\n",
      "Fold 30/64 patients, testing on ac05...\n",
      "Patient ac05 - Epoch 20/60, Train Loss: 0.6205, Train Acc: 0.6984\n",
      "Patient ac05 - Epoch 40/60, Train Loss: 0.4429, Train Acc: 0.8571\n",
      "Patient ac05 - Epoch 60/60, Train Loss: 0.4194, Train Acc: 0.8413\n",
      "Fold 31/64 patients, testing on ac07...\n",
      "Patient ac07 - Epoch 20/60, Train Loss: 0.5708, Train Acc: 0.7302\n",
      "Patient ac07 - Epoch 40/60, Train Loss: 0.4898, Train Acc: 0.8254\n",
      "Patient ac07 - Epoch 60/60, Train Loss: 0.4308, Train Acc: 0.8254\n",
      "Fold 32/64 patients, testing on ac08...\n",
      "Patient ac08 - Epoch 20/60, Train Loss: 0.5541, Train Acc: 0.7143\n",
      "Patient ac08 - Epoch 40/60, Train Loss: 0.4912, Train Acc: 0.7460\n",
      "Patient ac08 - Epoch 60/60, Train Loss: 0.3821, Train Acc: 0.8413\n",
      "Fold 33/64 patients, testing on ac09...\n",
      "Patient ac09 - Epoch 20/60, Train Loss: 0.5943, Train Acc: 0.6667\n",
      "Patient ac09 - Epoch 40/60, Train Loss: 0.5010, Train Acc: 0.8095\n",
      "Patient ac09 - Epoch 60/60, Train Loss: 0.4309, Train Acc: 0.8254\n",
      "Fold 34/64 patients, testing on ac10...\n",
      "Patient ac10 - Epoch 20/60, Train Loss: 0.6005, Train Acc: 0.6825\n",
      "Patient ac10 - Epoch 40/60, Train Loss: 0.4971, Train Acc: 0.7619\n",
      "Patient ac10 - Epoch 60/60, Train Loss: 0.4398, Train Acc: 0.8095\n",
      "Early stopping after 60 epochs\n",
      "Fold 35/64 patients, testing on ac11...\n",
      "Patient ac11 - Epoch 20/60, Train Loss: 0.5777, Train Acc: 0.7778\n",
      "Patient ac11 - Epoch 40/60, Train Loss: 0.4896, Train Acc: 0.8095\n",
      "Patient ac11 - Epoch 60/60, Train Loss: 0.4398, Train Acc: 0.8254\n",
      "Fold 36/64 patients, testing on ac12...\n",
      "Patient ac12 - Epoch 20/60, Train Loss: 0.5892, Train Acc: 0.7302\n",
      "Patient ac12 - Epoch 40/60, Train Loss: 0.4760, Train Acc: 0.7937\n",
      "Patient ac12 - Epoch 60/60, Train Loss: 0.3877, Train Acc: 0.8413\n",
      "Fold 37/64 patients, testing on ac13...\n",
      "Patient ac13 - Epoch 20/60, Train Loss: 0.5716, Train Acc: 0.7619\n",
      "Patient ac13 - Epoch 40/60, Train Loss: 0.4413, Train Acc: 0.7937\n",
      "Early stopping after 53 epochs\n",
      "Fold 38/64 patients, testing on ac14...\n",
      "Patient ac14 - Epoch 20/60, Train Loss: 0.5614, Train Acc: 0.6984\n",
      "Patient ac14 - Epoch 40/60, Train Loss: 0.4404, Train Acc: 0.7619\n",
      "Patient ac14 - Epoch 60/60, Train Loss: 0.3827, Train Acc: 0.8413\n",
      "Fold 39/64 patients, testing on ac15...\n",
      "Patient ac15 - Epoch 20/60, Train Loss: 0.5447, Train Acc: 0.7302\n",
      "Patient ac15 - Epoch 40/60, Train Loss: 0.4878, Train Acc: 0.8254\n",
      "Early stopping after 52 epochs\n",
      "Fold 40/64 patients, testing on ac16...\n",
      "Patient ac16 - Epoch 20/60, Train Loss: 0.5610, Train Acc: 0.7937\n",
      "Patient ac16 - Epoch 40/60, Train Loss: 0.4951, Train Acc: 0.7619\n",
      "Patient ac16 - Epoch 60/60, Train Loss: 0.4384, Train Acc: 0.7937\n",
      "Fold 41/64 patients, testing on ac17...\n",
      "Patient ac17 - Epoch 20/60, Train Loss: 0.5854, Train Acc: 0.7143\n",
      "Patient ac17 - Epoch 40/60, Train Loss: 0.5003, Train Acc: 0.7937\n",
      "Patient ac17 - Epoch 60/60, Train Loss: 0.4482, Train Acc: 0.8254\n",
      "Fold 42/64 patients, testing on ac18...\n",
      "Patient ac18 - Epoch 20/60, Train Loss: 0.5774, Train Acc: 0.7460\n",
      "Patient ac18 - Epoch 40/60, Train Loss: 0.4820, Train Acc: 0.8095\n",
      "Patient ac18 - Epoch 60/60, Train Loss: 0.4287, Train Acc: 0.8095\n",
      "Fold 43/64 patients, testing on ac19...\n",
      "Patient ac19 - Epoch 20/60, Train Loss: 0.5611, Train Acc: 0.7143\n",
      "Patient ac19 - Epoch 40/60, Train Loss: 0.5344, Train Acc: 0.7460\n",
      "Patient ac19 - Epoch 60/60, Train Loss: 0.4240, Train Acc: 0.8254\n",
      "Fold 44/64 patients, testing on ac20...\n",
      "Patient ac20 - Epoch 20/60, Train Loss: 0.5983, Train Acc: 0.6508\n",
      "Patient ac20 - Epoch 40/60, Train Loss: 0.4720, Train Acc: 0.8413\n",
      "Patient ac20 - Epoch 60/60, Train Loss: 0.4264, Train Acc: 0.8254\n",
      "Fold 45/64 patients, testing on ac21...\n",
      "Patient ac21 - Epoch 20/60, Train Loss: 0.6247, Train Acc: 0.6667\n",
      "Patient ac21 - Epoch 40/60, Train Loss: 0.4907, Train Acc: 0.8254\n",
      "Patient ac21 - Epoch 60/60, Train Loss: 0.4681, Train Acc: 0.7937\n",
      "Fold 46/64 patients, testing on ac22...\n",
      "Patient ac22 - Epoch 20/60, Train Loss: 0.5704, Train Acc: 0.7460\n",
      "Patient ac22 - Epoch 40/60, Train Loss: 0.4812, Train Acc: 0.7937\n",
      "Patient ac22 - Epoch 60/60, Train Loss: 0.4462, Train Acc: 0.8095\n",
      "Fold 47/64 patients, testing on ac23...\n",
      "Patient ac23 - Epoch 20/60, Train Loss: 0.6069, Train Acc: 0.6984\n",
      "Patient ac23 - Epoch 40/60, Train Loss: 0.4942, Train Acc: 0.8254\n",
      "Early stopping after 58 epochs\n",
      "Fold 48/64 patients, testing on ac24...\n",
      "Patient ac24 - Epoch 20/60, Train Loss: 0.5593, Train Acc: 0.7143\n",
      "Patient ac24 - Epoch 40/60, Train Loss: 0.4876, Train Acc: 0.7778\n",
      "Patient ac24 - Epoch 60/60, Train Loss: 0.4452, Train Acc: 0.8413\n",
      "Fold 49/64 patients, testing on ac25...\n",
      "Patient ac25 - Epoch 20/60, Train Loss: 0.5435, Train Acc: 0.7460\n",
      "Patient ac25 - Epoch 40/60, Train Loss: 0.4568, Train Acc: 0.8254\n",
      "Patient ac25 - Epoch 60/60, Train Loss: 0.4108, Train Acc: 0.8571\n",
      "Fold 50/64 patients, testing on ac26...\n",
      "Patient ac26 - Epoch 20/60, Train Loss: 0.5582, Train Acc: 0.6667\n",
      "Patient ac26 - Epoch 40/60, Train Loss: 0.4851, Train Acc: 0.8095\n",
      "Patient ac26 - Epoch 60/60, Train Loss: 0.4215, Train Acc: 0.8413\n",
      "Fold 51/64 patients, testing on ac27...\n",
      "Patient ac27 - Epoch 20/60, Train Loss: 0.5869, Train Acc: 0.7460\n",
      "Patient ac27 - Epoch 40/60, Train Loss: 0.4653, Train Acc: 0.7460\n",
      "Patient ac27 - Epoch 60/60, Train Loss: 0.4244, Train Acc: 0.8095\n",
      "Fold 52/64 patients, testing on ac28...\n",
      "Patient ac28 - Epoch 20/60, Train Loss: 0.5448, Train Acc: 0.7619\n",
      "Patient ac28 - Epoch 40/60, Train Loss: 0.4724, Train Acc: 0.8095\n",
      "Early stopping after 49 epochs\n",
      "Fold 53/64 patients, testing on ac29...\n",
      "Patient ac29 - Epoch 20/60, Train Loss: 0.5817, Train Acc: 0.7460\n",
      "Patient ac29 - Epoch 40/60, Train Loss: 0.4435, Train Acc: 0.8413\n",
      "Patient ac29 - Epoch 60/60, Train Loss: 0.4135, Train Acc: 0.8095\n",
      "Fold 54/64 patients, testing on ac30...\n",
      "Patient ac30 - Epoch 20/60, Train Loss: 0.5916, Train Acc: 0.7143\n",
      "Patient ac30 - Epoch 40/60, Train Loss: 0.4805, Train Acc: 0.7619\n",
      "Patient ac30 - Epoch 60/60, Train Loss: 0.4406, Train Acc: 0.8254\n",
      "Fold 55/64 patients, testing on ac32...\n",
      "Patient ac32 - Epoch 20/60, Train Loss: 0.5822, Train Acc: 0.7302\n",
      "Patient ac32 - Epoch 40/60, Train Loss: 0.5008, Train Acc: 0.8254\n",
      "Patient ac32 - Epoch 60/60, Train Loss: 0.4395, Train Acc: 0.8254\n",
      "Fold 56/64 patients, testing on ac33...\n",
      "Patient ac33 - Epoch 20/60, Train Loss: 0.5789, Train Acc: 0.6984\n",
      "Patient ac33 - Epoch 40/60, Train Loss: 0.4745, Train Acc: 0.7619\n",
      "Patient ac33 - Epoch 60/60, Train Loss: 0.4281, Train Acc: 0.8254\n",
      "Fold 57/64 patients, testing on ac34...\n",
      "Patient ac34 - Epoch 20/60, Train Loss: 0.5937, Train Acc: 0.6984\n",
      "Patient ac34 - Epoch 40/60, Train Loss: 0.4789, Train Acc: 0.8095\n",
      "Early stopping after 59 epochs\n",
      "Fold 58/64 patients, testing on ac37...\n",
      "Patient ac37 - Epoch 20/60, Train Loss: 0.5775, Train Acc: 0.7460\n",
      "Patient ac37 - Epoch 40/60, Train Loss: 0.5021, Train Acc: 0.7778\n",
      "Patient ac37 - Epoch 60/60, Train Loss: 0.4389, Train Acc: 0.8413\n",
      "Fold 59/64 patients, testing on ac38...\n",
      "Patient ac38 - Epoch 20/60, Train Loss: 0.5510, Train Acc: 0.7302\n",
      "Patient ac38 - Epoch 40/60, Train Loss: 0.4463, Train Acc: 0.8095\n",
      "Patient ac38 - Epoch 60/60, Train Loss: 0.3914, Train Acc: 0.8413\n",
      "Fold 60/64 patients, testing on ac39...\n",
      "Patient ac39 - Epoch 20/60, Train Loss: 0.5705, Train Acc: 0.6984\n",
      "Patient ac39 - Epoch 40/60, Train Loss: 0.5357, Train Acc: 0.7302\n",
      "Patient ac39 - Epoch 60/60, Train Loss: 0.4681, Train Acc: 0.7937\n",
      "Fold 61/64 patients, testing on CAR122...\n",
      "Patient CAR122 - Epoch 20/60, Train Loss: 0.5619, Train Acc: 0.6825\n",
      "Patient CAR122 - Epoch 40/60, Train Loss: 0.4985, Train Acc: 0.7460\n",
      "Patient CAR122 - Epoch 60/60, Train Loss: 0.4886, Train Acc: 0.7778\n",
      "Fold 62/64 patients, testing on CAR128...\n",
      "Patient CAR128 - Epoch 20/60, Train Loss: 0.5699, Train Acc: 0.6667\n",
      "Patient CAR128 - Epoch 40/60, Train Loss: 0.4761, Train Acc: 0.7619\n",
      "Patient CAR128 - Epoch 60/60, Train Loss: 0.3831, Train Acc: 0.8413\n",
      "Fold 63/64 patients, testing on CAR139...\n",
      "Patient CAR139 - Epoch 20/60, Train Loss: 0.5828, Train Acc: 0.7460\n",
      "Patient CAR139 - Epoch 40/60, Train Loss: 0.4658, Train Acc: 0.8095\n",
      "Patient CAR139 - Epoch 60/60, Train Loss: 0.4180, Train Acc: 0.8413\n",
      "Fold 64/64 patients, testing on CAR142...\n",
      "Patient CAR142 - Epoch 20/60, Train Loss: 0.5700, Train Acc: 0.7143\n",
      "Patient CAR142 - Epoch 40/60, Train Loss: 0.5127, Train Acc: 0.7143\n",
      "Patient CAR142 - Epoch 60/60, Train Loss: 0.4360, Train Acc: 0.7937\n",
      "\n",
      "===== LOOCV Final Results =====\n",
      "Overall Accuracy: 0.7031 (45/64 patients correct)\n",
      "Overall Precision: 0.7105\n",
      "Overall Recall: 0.7714\n",
      "Overall F1 Score: 0.7397\n",
      "Overall AUC: 0.6660\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18 11]\n",
      " [ 8 27]]\n",
      "\n",
      "Class-specific Metrics:\n",
      "class_0: Precision=0.6923, Recall=0.6207, Count=29\n",
      "class_1: Precision=0.7105, Recall=0.7714, Count=35\n",
      "Pipeline completed successfully! Results saved to results/run_20250416_165918\n"
     ]
    }
   ],
   "source": [
    "# more complex sample classifier\n",
    "# added class weight\n",
    "# increased weight decay to 1e-3\n",
    "# added layer norm to feature extractor\n",
    "# removed layer norm to feature extractor, added layer norm to autoencoder\n",
    "results = run_pipeline_loocv(\n",
    "    input_file=\"input_scenic_data/cell_atlas_axicel_IP_scenic_tf_matrix_added_v2.h5ad\",\n",
    "    output_dir=\"results\", \n",
    "    latent_dim=64, #48,\n",
    "    num_epochs_ae=150, \n",
    "    num_epochs=60,\n",
    "    num_classes=2,\n",
    "    hidden_dim=128,\n",
    "    sample_source_dim=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b513fea-5764-4e88-bcc0-823434d0b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save to file\n",
    "with open('MIL_downstream_70.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "    \n",
    "# Load from file\n",
    "# with open('MIL_downstream.pkl', 'rb') as f:\n",
    "#     loaded_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41637fda-c218-4beb-85fe-3f22fd08b339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['adata', 'autoencoder', 'latent_data', 'mil_results', 'results_dir'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3edbd94-124b-4219-a3f6-bec3ab858aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the label and prediction from the patient_predictions\n",
    "label_list = [int(pred['true_label']) for pred in results['mil_results']['patient_predictions'].values()]\n",
    "prediction_list = [pred['predicted_label'] for pred in results['mil_results']['patient_predictions'].values()]\n",
    "probability_list = [prob['probabilities'][1] for prob in results['mil_results']['patient_predictions'].values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bba0b790-30b5-49a4-b157-37c8b0cd71c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'True Positive Rate')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot roc curve\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "# calculate the roc curve\n",
    "fpr, tpr, thresholds = roc_curve(label_list, probability_list)\n",
    "\n",
    "# plot the roc curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "481719a8-566a-49d5-8589-96736dc3f8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6960591133004925"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(label_list, prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d62e2b-90bb-488f-a8aa-94c5688123d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scVI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
